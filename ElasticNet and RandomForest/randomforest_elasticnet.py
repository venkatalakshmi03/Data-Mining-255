# -*- coding: utf-8 -*-
"""RandomForest_ElasticNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dsvWr822R92O85usG4bA4EmD5CwqKeSX
"""

import os
import random 

import pandas as pd 
import numpy as np 
from scipy.stats import kurtosis, skew 
from scipy import stats

import matplotlib.pyplot as plt 
import seaborn as sns
# Importing librarys to use on interactive graphs
import plotly.offline as plty
from plotly import tools as tls
import plotly.express as px
from plotly.offline import init_notebook_mode, iplot, plot 
import plotly.graph_objs as go 
import time
from sklearn import model_selection, preprocessing, metrics
import json # to convert json in df
from pandas import json_normalize # to normalize the json file
from datetime import datetime
from sklearn.feature_selection import SelectKBest, chi2
import lightgbm as lgb
import xgboost as xgb

"""mounting gdrive for data"""

from google.colab import drive
drive.mount('/content/gdrive')

!unzip -q "./gdrive/MyDrive/cmpe255/train.csv.zip"

df_train = pd.read_csv("./train.csv")

df_train.head()

df_train.info()

columns = ['device', 'geoNetwork', 'totals', 'trafficSource']
def json_read(df):
    df = pd.read_csv(df, converters={column: json.loads for column in columns},dtype={'fullVisitorId': 'str'}) 
    for column in columns: 
        column_as_df = json_normalize(df[column]) 
        column_as_df.columns = [f"{column}.{subcolumn}" for subcolumn in column_as_df.columns] 
        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)
    return df

df = json_read("./train.csv")

df # 900000 rows with 55 columns

df.isnull().sum()

df.info()

df['totals.pageviews'].mode()

columns_to_remove = [col for col in df.columns if df[col].nunique() == 1]
print("No. of variables with unique value: {}".format(len(columns_to_remove)))

columns_to_remove

for col in columns_to_remove:
    if set(['not available in demo dataset']) ==  set(df[col].unique()): continue
    print(col, df[col].dtypes, df[col].unique())

df_1 = df.copy()

df_1.head()

df_1['totals.pageviews'].fillna(1, inplace=True) # mode of page views is 1
df_1['totals.newVisits'].fillna(0, inplace=True)
df_1['totals.bounces'].fillna(0, inplace=True) 
df_1["totals.transactionRevenue"].fillna(0.0, inplace=True)

#Changing objects to int
df_1['totals.pageviews'] = df_1['totals.pageviews'].astype(int)
df_1['totals.newVisits'] = df_1['totals.newVisits'].astype(int)
df_1['totals.bounces'] = df_1['totals.bounces'].astype(int)
df_1["totals.transactionRevenue"] = df_1["totals.transactionRevenue"].astype(float)

df_1['trafficSource.isTrueDirect'].fillna(False, inplace=True) 
df_1['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True) # filling boolean with True
df_1[df_1['geoNetwork.city'] == "(not set)"]['geoNetwork.city'] = np.nan
df_1['geoNetwork.city'].fillna("NaN", inplace=True)

df_1

#df_1[df_1['geoNetwork.city'] == "(not set)"]['geoNetwork.city'] = np.nan
#df_1['geoNetwork.city'].fillna("NaN", inplace=True)

"""**Exploring the totals.transactionRevenue variable in the dataset**"""

#len(df_1)
#totalusers = df_1["fullVisitorId"].count()
#rdf= df_1.groupby('fullVisitorId')["totals.transactionRevenue"].sum().reset_index();
#nrgu = rdf[rdf['totals.transactionRevenue'] == 0]
#rgu = rdf[rdf['totals.transactionRevenue'] > 0]

#print("The total number of users", totalusers)
#print("The number of non revenue generating users are ", len(nrgu))
#print("The number of revenue generating users are ", len(rgu))

#labels = ['Non revenue generating users', 'Revenue generating users']
#values = [len(nrgu), len(rgu)]
#colors = ["maroon", "yellow"]
#fig = go.Figure(data=[go.Pie(labels=labels, values=values,pull=[0, 0.5])])
#fig.update_traces(rotation=90, selector=dict(type='pie'),title = "Revenue Generating Users Vs Non-revenue Generating Users")
#fig.show()
rdf= df_1.groupby('fullVisitorId')["totals.transactionRevenue"].sum().reset_index()
#print(r1)
figtrace = go.Scatter(
   x=df_1.index,
    y=np.sort(df_1['totals.transactionRevenue'].values),
    mode='markers',
    marker=dict(
        sizemode="diameter",
        sizeref=2,
        size=4,
        color=df_1['totals.transactionRevenue'].values,
        colorscale="Plotly3",
        showscale=True
    )
)
data = [figtrace]
figlayout=go.Layout(title="Transaction Revenue Range")
figscatter=go.Figure(data=data,layout=figlayout)
#iplot(figscatter, filename="tan")
figscatter.show()

'''plt.figure(figsize=(8,6))
plt.scatter(range(df_1.shape[0]), np.sort(df_1['totals.transactionRevenue'].values.astype(float)), color="#C71585")
plt.xlabel('index', fontsize=15)
plt.ylabel('Transaction Revenue', fontsize=15)
plt.show()'''

###figpx=px.line(x=rdf.index, y=np.sort(rdf['totals.transactionRevenue'].values), labels={'x': 'index', 'y':'Transaction Revenue'}, color="#C71585")
#figpx.show()'''

"""80/20 rule mentioned in the dataset. only 20% customer contribute towards 80% of the revenue.

**Logarithmic Distribution of Non-Zero Transaction Revenue**
"""

import plotly.figure_factory as ff
#df_tran = df_1.copy
#df_tran

non_zero_revenue = df_1[df_1["totals.transactionRevenue"] > 0]["totals.transactionRevenue"]
log_non_zero_revenue = np.log1p(non_zero_revenue)
hist_data = [log_non_zero_revenue]
group_labels = ['Total Non-Zero Transaction Revenue']
colors = ["Orchid"]
fig1 = ff.create_distplot(hist_data, group_labels, colors=colors,show_rug=False)
fig1.update_layout(title_text = "Logarithmic Distribution of Total Non Zero Transaction Revenue")
fig1.show()

""" Not a proper bell curve"""

tgt_var = log_non_zero_revenue
print("The skew of the transaction revenue value:", skew(tgt_var))
print("The kurtosis of the transaction revenue value:", (kurtosis(tgt_var)))

"""**Explore the distribution of the sum of the log transaction revenue per users .**

"""

sum_rev_per_user = rdf
log_sum_rev_per_user = np.log(sum_rev_per_user.loc[sum_rev_per_user['totals.transactionRevenue'] > 0, 'totals.transactionRevenue'])
hist_data_1 = [log_non_zero_revenue]
colors_a = ["YellowGreen"]
group_labels = ['Transaction Revenue per user']
fig2 = ff.create_distplot(hist_data, group_labels, colors=colors_a,show_rug=False)
fig2.update_layout(title_text = "Logarithmic Distribution of Total Non Zero Transaction Revenue Per User")
fig2.show()

print("The skew of the transaction revenue value:", skew(log_sum_rev_per_user))
print("The kurtosis of the transaction revenue value:", (kurtosis(log_sum_rev_per_user)))

"""Exploring the geoNetwork.city attribute th
**Top Cities By Visits**
"""

city_val = ["(not set)", "not available in demo dataset"]
city_not_set = df_1 [~df_1["geoNetwork.city"].isin(city_val)]
city_not_set
city_count = city_not_set["geoNetwork.city"].value_counts()[:10].to_frame().reset_index()
city_fig = go.Pie(labels=city_count['index'], values=city_count["geoNetwork.city"], name= "% Visits", hole= .5, hoverinfo="label+percent+name", showlegend=True,domain= {'x': [.52, .1]})
city_layout = dict(title= "Top 10 Cities By Visits", height=450, font=dict(size=15),
             annotations = [
                      dict(
                          x=0.50, y=.5,
                          text='Counts', 
                          showarrow=False,
                          font=dict(size=20)
                      ),
                  ])
top_city_fig = dict( data=[city_fig], layout = city_layout)
iplot(top_city_fig)

"""**Top cities based on Revenue**"""

city_rev_group = city_not_set.groupby("geoNetwork.city")["totals.transactionRevenue"].sum().nlargest(8).to_frame().reset_index()
city_rev_fig = go.Pie(labels=city_rev_group["geoNetwork.city"], values=city_rev_group['totals.transactionRevenue'], name= "% Revenue", hole= .5, 
                    hoverinfo="label+percent+name", showlegend=True,domain= {'x': [.52, .1]})
rev_city_layout = dict(title= "Top 8 Cities based on Transaction Revenue", height=450, font=dict(size=15),
                  annotations = [
                      dict(
                          x=.50, y=.5,
                          text='Revenue', 
                          showarrow=False,
                          font=dict(size=20)
                      ),
                  ])
top_city_rev_fig = dict( data=[city_rev_fig], layout = rev_city_layout)
iplot(top_city_rev_fig)

"""**Channel Grouping By Revenue**"""

channel_group = df_1.groupby("channelGrouping")["totals.transactionRevenue"].sum().reset_index()
'''channeltrace = go.Bar(
    x=channel_group['channelGrouping'].values,
    y=channel_group['totals.transactionRevenue'].values.astype(float),
)
layout = go.Layout(title="Channel Grouping vs Revenue")
data = [channeltrace]
fig = go.Figure(data=data, layout=layout)
iplot(fig, filename="channelgrouping")'''
#channelfig=px.bar(channel_group, x='channelGrouping', y='totals.transactionRevenue')
#channelfig.update_xaxes(type ='category')
#channelfig.show()
channeltrace = go.Pie(labels=channel_group['channelGrouping'], values=channel_group['totals.transactionRevenue'], name="% revenue", hoverinfo="label+percent+name")
layout=go.Layout(title="Channel Grouping vs Revenue")
data=[channeltrace]
channelfig=go.Figure(data=data, layout=layout)
channelfig.show()
#channel_group

df_1.isnull().sum()

columns = [col for col in df_1.columns if df_1[col].nunique() > 1]
df_1 = df_1[columns]

df_1.info()

df_1.isnull().sum()

# Removing columns with high null values 
df_1.drop(columns=['trafficSource.adContent', 'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adwordsClickInfo.gclId',
                'trafficSource.adwordsClickInfo.slot', 'trafficSource.adwordsClickInfo.page', 'trafficSource.referralPath',
                 'trafficSource.keyword'], inplace = True)

print(df.shape)
print(df_1.shape)

df_1.info()

"""# Copy into another dataframe to complete feature Engineering"""

df_2 = df_1.copy()

df_2.info()

# Visit time
df_2['diff_visitId_time'] = df_2['visitId'] - df_2['visitStartTime']
df_2['diff_visitId_time'] = (df_2['diff_visitId_time'] != 0).astype(int)
del df_2['visitId']

# Visit hour
df_2['formated_visitStartTime'] = df_2['visitStartTime'].apply(
    lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x)))
df_2['formated_visitStartTime'] = pd.to_datetime(df_2['formated_visitStartTime'])
df_2['visit_hour'] = df_2['formated_visitStartTime'].apply(lambda x: x.hour)
 
del df_2['visitStartTime']
del df_2['formated_visitStartTime']

# date
format_str = '%Y%m%d' 
df_2['formated_date'] = df_2['date'].apply(lambda x: datetime.strptime(str(x), format_str))
df_2['month'] = df_2['formated_date'].apply(lambda x:x.month)
df_2['quarter_month'] = df_2['formated_date'].apply(lambda x:x.day//8)
df_2['day'] = df_2['formated_date'].apply(lambda x:x.day)
df_2['weekday'] = df_2['formated_date'].apply(lambda x:x.weekday())
 
del df_2['date']

# total hits
df_2['totals.hits'] = df_2['totals.hits'].astype(int)
df_2['mean_hits_per_day'] = df_2.groupby(['day'])['totals.hits'].transform('mean')
del  df_2['day']

num_cols = ["totals.hits", "totals.pageviews", "visitNumber", 'totals.bounces',  'totals.newVisits']    
for col in num_cols:
    df_2[col] = df_2[col].astype(float)

# Removing unwanted id columns
df_2 = df_2.drop(['fullVisitorId','sessionId'], axis = 1)

df_2.info()

df_2.head()

#Copy for Encoding Categorical Features
df_3 = df_2.copy()

df_3.info()

cat_cols = ["channelGrouping", "device.browser", 
             "device.operatingSystem", 'device.isMobile','device.deviceCategory',
            "geoNetwork.city", "geoNetwork.continent", 
            "geoNetwork.country", "geoNetwork.metro",
            "geoNetwork.networkDomain", "geoNetwork.region", 
            "geoNetwork.subContinent", 
             "trafficSource.campaign",
            "trafficSource.medium", 
             "trafficSource.source",
            'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.isTrueDirect']

#Label Encoding
for col in cat_cols:
    # print(col)
    lbl = preprocessing.LabelEncoder()
    lbl.fit(list(df_3[col].values.astype('str')))
    df_3[col] = lbl.transform(list(df_3[col].values.astype('str')))

#Converting to numerical Columns
num_cols = ["totals.hits", "totals.pageviews", 
            "visitNumber", 
            'totals.bounces',  'totals.newVisits']    

for col in num_cols:
    df_3[col] = df_3[col].astype(float)

df_3.info()

df_3.head()

df_3['visit_hour_sin'] = np.sin(df_3.visit_hour*(2.*np.pi/24))
df_3['visit_hour_cos'] = np.cos(df_3.visit_hour*(2.*np.pi/24))
df_3['month_sin'] = np.sin((df_3.month-1)*(2.*np.pi/12))
df_3['month_cos'] = np.cos((df_3.month-1)*(2.*np.pi/12))

df_3['weekday_sin'] = np.sin((df_3.weekday-1)*(2.*np.pi/7))
df_3['weekday_cos'] = np.cos((df_3.weekday-1)*(2.*np.pi/7))

del df_3['month']
del df_3['visit_hour']

del df_3['weekday']

df_3.head()

plt.rcParams["figure.figsize"] = (30,30)

sns.heatmap(df_3.corr(), annot = True)

"""Feature Engineering Completed. # Dividing into training and testing"""

X = df_3.copy()

X.shape

X.head()

X.info()

from datetime import datetime, date

X = X.set_index(X['formated_date'])
X = X.sort_index()

min(X['formated_date'])

max(X['formated_date'])

train = X['2016-08-01':'2017-05-31']
test  = X['2017-05-31':]
print(train.shape)
print(test.shape)

# Dropping date and target column
X_train = train.drop(['formated_date','totals.transactionRevenue'], axis = 1)
X_test = test.drop(['formated_date','totals.transactionRevenue'], axis=1)
print(X_train.shape)
print(X_test.shape)

X_train.head()

out = test['totals.transactionRevenue'].values

output = pd.DataFrame(out)

output.columns = ['ActualRevenue']

output.head()

#  transaction revenue
y_train = np.log1p(train["totals.transactionRevenue"].values)
y_test = np.log1p(test["totals.transactionRevenue"].values)
print(y_train.shape)
print(y_test.shape)

from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV

#Elastic Net model with default parameters
en_model = ElasticNet()
en_model.fit(X_train, y_train)

#print(np.sqrt(metrics.mean_squared_error(y_test, pred_ridge)))

pred_revenue_en = en_model.predict(X_test)
pred_revenue_en

pred_revenue_en[pred_revenue_en < 0] = 0
output["Predicted Revenue"] = np.expm1(pred_revenue_en)
output[["ActualRevenue", "Predicted Revenue"]].head()

pd.options.display.float_format = '{:.2f}'.format

output[output['ActualRevenue'] > 0].head()

print("ElasticNet- RMSE without hypertuning", np.sqrt(metrics.mean_squared_error(y_test, pred_revenue_en)))

#Tuning the parameters
en_model_1 = ElasticNet(alpha=0.5, l1_ratio=0.3)
en_model_1.fit(X_train, y_train)
pred_rev_enp = en_model_1.predict(X_test)
print("ElasticNet RMSE for alpha= 0.5 and l1_ratio = 0.3: ", np.sqrt(metrics.mean_squared_error(y_test, pred_rev_enp)))

# alpha = 0.1, l1_ratio = 0.3
en_model_2 = ElasticNet(alpha = 0.1, l1_ratio=0.3)
en_model_2.fit(X_train, y_train)
pred_rev_2= en_model_2.predict(X_test)
print("ElasticNet RMSE for alpha= 0.1 and l1_ratio = 0.3: ", np.sqrt(metrics.mean_squared_error(y_test,  pred_rev_2)))

#alpha = 0.1, l1_ratio = 0.1
en_model_ratio = ElasticNet(alpha = 0.1, l1_ratio=0.1)
en_model_ratio.fit(X_train,y_train)
pred_rev_r1 =en_model_ratio.predict(X_test)
print("ElasticNet RMSE for alpha= 0.1 and l1_ratio = 0.1: ", np.sqrt(metrics.mean_squared_error(y_test, pred_rev_r1)))

#alpha = 0.01, l1_ratio = 0.1
en_model_3 = ElasticNet(alpha=0.01, l1_ratio = 0.1)
en_model_3.fit(X_train,y_train)
pred_rev_3 = en_model_3.predict(X_test)
print("ElasticNet RMSE for alpha= 0.01 and l1_ratio = 0.1: ", np.sqrt(metrics.mean_squared_error(y_test, pred_rev_3)))

"""**Feature Importance for Elastic Net Model**"""

Coloumn_Coeff= [(X_train.columns[i], en_model_3.coef_ [i]) for i in range(len(en_model_3.coef_))]
Coloumn_Coeff=sorted(Coloumn_Coeff, key=lambda x:  (x[1]),reverse=True)
Coloumn_Coeff
column = []
val = []
for v in Coloumn_Coeff:
  if v[1] > 0.01:
    column.append(v[0])
    val.append(v[1])
val
column
importance = pd.Series(index = column, data = val)
importance.plot(kind='bar', title = "Feature Importance for ElasticNet Model with alpha = 0.01 and l1_ratio=0.1", xlabel= "Features", ylabel="Coefficient Values", colormap="rainbow", figsize = (10,10),fontsize=12)

"""**Random Forest Regressor Model**"""

random_model_base = RandomForestRegressor()
random_model_base.fit(X_train,y_train)
pred_rev_rf = random_model_base.predict(X_test)
print("RMSE for RandomForest Regressor without hypertuning", np.sqrt(metrics.mean_squared_error(y_test, pred_rev_rf)))

#Random Forest model with parameter tuning
#random_model_1 = RandomForestRegressor(max_depth=11,random_state=0,n_estimators=100)
#random_model_1.fit(X_train, y_train)
#pred_rev_random= random_model.predict(X_test)

#print("RMSE for RandomForest Regressor with max_depth = 11, n_estimators=100", np.sqrt(metrics.mean_squared_error(y_test, pred_rev_random)))

#RandomForest Regressor Model with different parameter values
#random_model_2 = RandomForestRegressor(max_depth=8,min_samples_split=400, n_estimators=10,random_state=0)
#random_model_2.fit(X_train, y_train)
#pred_rev_random1 = random_model_2.predict(X_test)
#print("RMSE for RandomForest Regressor with n_estimators-10, max_depth=8,min_samples_split=400: ", np.sqrt(metrics.mean_squared_error(y_test, pred_rev_random1)))

#Random Forest with Grid Search Cross Validation
from sklearn.model_selection import GridSearchCV
from pprint import pprint

param_grid = {
    'bootstrap': [True],
    'max_depth': [80,90,100,110],
    'max_features': [2, 3],
    'min_samples_split': [8, 10, 12],
    'min_samples_leaf': [3, 4, 5],
    'n_estimators': [100, 200, 300, 1000]
}

pprint(param_grid)

#base model with grid and gridsearch with param_grid
random_model_base = RandomForestRegressor()
random_grid = GridSearchCV(estimator=random_model_base, param_grid=param_grid, n_jobs=-1, cv=3, verbose=2)

#fit data
random_grid.fit(X_train, y_train)

#find best parameters
random_grid.best_params_

#find best estimator 
random_best = random_grid.best_estimator_
random_best

#fit the data with the best estimator
random_best.fit(X_train, y_train)

#Predict using the best estimator value
pred_random_best = random_best.predict(X_test)
print("RMSE for RandomForest Regressor  best_param values n_estimators=1000, min_samples_split = 8, max_depth = 80:", np.sqrt(metrics.mean_squared_error(y_test, pred_random_best)))

"""Feature Importance for Random Forest Regressor"""

feature_importance = random_model_base.feature_importances_
imp_pair= [(X_train.columns[i], feature_importance[i]) for i in range(len(feature_importance))]
imp_pair=sorted(imp_pair, key=lambda x:  (x[1]),reverse=True)
imp_pair
feature = []
imp_val = []
for pair in imp_pair:
  if(pair[1]> 0.01):
    feature.append(pair[0])
    imp_val.append(pair[1])
importance_random = pd.Series(index = feature, data = imp_val)
importance_random.plot(kind='bar', title = "Feature Importance for Random Forest Regressor ", xlabel= "Features", ylabel="Importance", colormap="Accent", figsize = (10,7),fontsize=14)

feature_best = random_best.feature_importances_
feature_best
best_imp= [(X_train.columns[i], feature_best[i]) for i in range(len(feature_best))]
best_imp=sorted(best_imp, key=lambda x:  (x[1]),reverse=True)
best_feature = []
best_val = []
for best in best_imp:
  if(best[1]> 0.01):
    best_feature.append(best[0])
    best_val.append(best[1])
importance_best = pd.Series(index = best_feature, data = best_val)
importance_best.plot(kind='bar', title = "Feature Importance for Random Forest Regressor (best parameters)", xlabel= "Features", ylabel="Importance", colormap="Accent", figsize = (10,7),fontsize=14)

X_train.columns